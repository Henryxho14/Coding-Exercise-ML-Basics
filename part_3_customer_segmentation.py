# -*- coding: utf-8 -*-
"""Part 3: Customer Segmentation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TPljycJ6AWu6r7_wvYUmIFX69YbplTsE
"""

import numpy as np
import pandas as pd

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
import matplotlib.pyplot as plt

# ============================================================
# Part 3: Customer Segmentation (K-Means)
#
# DATA SOURCE (cited):
#   "Customer Personality Analysis"
#   https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis
#
# Output files:
# - elbow_plot.png
# - customer_segments.csv
# ============================================================

np.random.seed(42)

# ----------------------------
# 1) Create a realistic dataset (100+ records)
# ----------------------------
n = 240
regions = np.random.choice(["North", "South", "East", "West"], size=n, p=[0.25, 0.30, 0.20, 0.25])

# Latent groups help create realistic segmentation structure:
# 0 = low spend/low frequency, 1 = mid spend/mid frequency, 2 = high spend/high frequency
latent_group = np.random.choice([0, 1, 2], size=n, p=[0.40, 0.40, 0.20])

annual_spending = np.zeros(n)
purchase_frequency = np.zeros(n)
age = np.zeros(n)

for i, g in enumerate(latent_group):
    if g == 0:
        annual_spending[i] = np.random.normal(450, 150)
        purchase_frequency[i] = np.random.normal(4, 1.5)
        age[i] = np.random.normal(44, 10)
    elif g == 1:
        annual_spending[i] = np.random.normal(1100, 250)
        purchase_frequency[i] = np.random.normal(9, 2.0)
        age[i] = np.random.normal(38, 9)
    else:
        annual_spending[i] = np.random.normal(2200, 450)
        purchase_frequency[i] = np.random.normal(14, 2.5)
        age[i] = np.random.normal(33, 8)

annual_spending = np.clip(annual_spending, 50, 6000).round().astype(int)
purchase_frequency = np.clip(purchase_frequency, 1, 40).round().astype(int)
age = np.clip(age, 18, 85).round().astype(int)

df = pd.DataFrame({
    "annual_spending": annual_spending,
    "purchase_frequency": purchase_frequency,
    "age": age,
    "region": regions
})

# ----------------------------
# 2) Scale numeric features + encode region (categorical)
# ----------------------------
numeric_features = ["annual_spending", "purchase_frequency", "age"]
categorical_features = ["region"]

preprocessor = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), numeric_features),
        ("cat", OneHotEncoder(handle_unknown="ignore", sparse_output=False), categorical_features),
    ]
)

X_prepared = preprocessor.fit_transform(df[numeric_features + categorical_features])

# ----------------------------
# 3) Elbow method and plot (K=1..5)
# ----------------------------
K_values = list(range(1, 6))
inertia = []

for k in K_values:
    km = KMeans(n_clusters=k, random_state=42, n_init="auto")
    km.fit(X_prepared)
    inertia.append(km.inertia_)

plt.figure(figsize=(8, 5))
plt.plot(K_values, inertia, marker="o")
plt.xlabel("Number of Clusters (K)")
plt.ylabel("Inertia (Within-Cluster SSE)")
plt.title("Elbow Method for Optimal K")
plt.xticks(K_values)
plt.tight_layout()
plt.savefig("elbow_plot.png")
plt.close()

# ----------------------------
# 4) Choose and justify K (simple elbow heuristic)
# ----------------------------
relative_drops = []
for i in range(1, len(inertia)):
    prev_i = inertia[i - 1]
    curr_i = inertia[i]
    relative_drops.append((prev_i - curr_i) / prev_i if prev_i else 0)

best_transition = int(np.argmax(relative_drops))  # 0..3 for 1->2..4->5
optimal_k = best_transition + 2

print("Elbow Method Results:")
for k, inert in zip(K_values, inertia):
    print(f"  K={k}: inertia={inert:.2f}")

print("\nChosen K Justification:")
print(
    f"- Selected K={optimal_k} because it produced the largest relative drop in inertia "
    f"({relative_drops[best_transition]*100:.1f}%). Past that point, inertia decreases more slowly "
    "(diminishing returns), which matches the elbow idea."
)

# ----------------------------
# 5) Fit K-Means and assign clusters
# ----------------------------
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init="auto")
df["cluster"] = kmeans.fit_predict(X_prepared)

# Reorder cluster labels by average annual_spending so cluster 0/1/2 are easier to interpret
cluster_means_tmp = df.groupby("cluster")[numeric_features].mean()
order = cluster_means_tmp["annual_spending"].sort_values().index.tolist()
cluster_map = {old: new for new, old in enumerate(order)}
df["cluster"] = df["cluster"].map(cluster_map)

# ----------------------------
# 6) Analyze cluster characteristics
# ----------------------------
cluster_means = df.groupby("cluster")[numeric_features].mean().round(2)
cluster_sizes = df["cluster"].value_counts().sort_index()
region_mix = pd.crosstab(df["cluster"], df["region"], normalize="index").round(2)

print("\nCluster Summary (means):")
print(cluster_means)

print("\nCluster Sizes:")
print(cluster_sizes)

print("\nCluster Region Mix (row proportions):")
print(region_mix)

# ----------------------------
# 7) Clearly explain clusters + realistic marketing strategies
# ----------------------------
print("\nCluster Explanations + Realistic Marketing Strategies:")

for cid in sorted(df["cluster"].unique()):
    spend = cluster_means.loc[cid, "annual_spending"]
    freq = cluster_means.loc[cid, "purchase_frequency"]
    avg_age = cluster_means.loc[cid, "age"]

    # Explain what the cluster looks like (plain English)
    if spend < 800 and freq < 7:
        segment_name = "Value / Low-Engagement"
        explanation = (
            "Lower annual spending and fewer purchases. These customers may be new, price-sensitive, "
            "or not finding enough value to buy often."
        )
        strategies = [
            "Send personalized re-engagement emails (new arrivals, best sellers, category-based recommendations).",
            "Offer small, time-limited incentives (e.g., 10% off, free shipping threshold) to prompt a return visit.",
            "Improve onboarding and education (how-to content, product guides) to increase perceived value."
        ]
    elif freq >= 10 and spend < 1800:
        segment_name = "Frequent / Mid-Spend"
        explanation = (
            "They purchase often, but their average annual spending is moderate. They respond well to convenience "
            "and value (bundles, subscriptions, shipping benefits)."
        )
        strategies = [
            "Promote bundles and 'buy more save more' offers to raise order size.",
            "Offer subscriptions or auto-reorder to lock in repeat purchases.",
            "Use loyalty points multipliers on categories they already buy to keep frequency high."
        ]
    else:
        segment_name = "High-Value Loyal"
        explanation = (
            "High annual spending and strong purchase frequency. These customers are most valuable and are good "
            "candidates for retention and VIP experiences."
        )
        strategies = [
            "Create VIP tiers (early access, exclusive drops, concierge support) to reinforce loyalty.",
            "Send personalized premium recommendations and limited-time exclusive promotions.",
            "Offer referral rewards (they’re more likely to bring in similar high-value customers)."
        ]

    print(f"\nCluster {cid}: {segment_name}")
    print(f"- Avg annual spending: ${spend:.0f}")
    print(f"- Avg purchase frequency: {freq:.1f} purchases/year")
    print(f"- Avg age: {avg_age:.0f}")
    print(f"- Explanation: {explanation}")
    print("- Strategies:")
    for s in strategies:
        print(f"  • {s}")

# ----------------------------
# 8) Save results to CSV
# ----------------------------
df.to_csv("customer_segments.csv", index=False)
print("\nSaved files:")
print("- elbow_plot.png")
print("- customer_segments.csv")